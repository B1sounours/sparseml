{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Â© 2021-present Neuralmagic, Inc. // Neural Magic Legal\n",
    "\n",
    "Torchvision Classification Model Pruning using SparseML\n",
    "This notebook provides a step-by-step walkthrough for pruning a torchvision model using SparseML. You will:\n",
    "\n",
    "Download a pre-trained torchvision model and generic dataset\n",
    "Define a generic torchvision finetuning flow\n",
    "Integrate the torchvision flow with SparseML\n",
    "Prune the model using the torchvision+SparseML flow\n",
    "Save the model and export to ONNX\n",
    "Reading through this notebook will be reasonably quick to gain an intuition for how to integrate SparseML with torchvision or more generically a PyTorch training flow. Rough time estimates for fully pruning the default model are given. Note that training with the PyTorch CPU implementation will be much slower than a GPU:\n",
    "\n",
    "15 minutes on a GPU\n",
    "45 minutes on a laptop CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1 - Requirements\n",
    "To run this notebook, you will need the following packages already installed:\n",
    "\n",
    "SparseML and SparseZoo\n",
    "PyTorch and torchvision\n",
    "You can install any package that is not already present via pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating Config for BERT-QA GMP training/pruning with Neural Magic\n",
    "\"\"\"\n",
    "\n",
    "notebook_name = \"BERTQA-NM\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Union, Tuple, Iterable, Dict, Any, List\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "import wandb\n",
    "from glob import glob\n",
    "from tqdm import auto\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from onnx import ModelProto\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Linear, Parameter\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoConfig, AutoModelForQuestionAnswering,\n",
    "                          AutoTokenizer, DataCollatorWithPadding,\n",
    "                          PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.pytorch.recal import approx_ks_loss_sensitivity\n",
    "from neuralmagicML.utilsnb import check_pytorch_notebook_setup\n",
    "from neuralmagicML.pytorch.utils import CrossEntropyLossWrapper\n",
    "from neuralmagicML.utils import create_unique_dir, clean_path\n",
    "from neuralmagicML.pytorch.utils import (\n",
    "    CrossEntropyLossWrapper,\n",
    "    TopKAccuracy,\n",
    "    ModuleTrainer,\n",
    "    ModuleTester,\n",
    "    TensorBoardLogger,\n",
    ")\n",
    "\n",
    "from neuralmagicML.pytorch.utils import ModuleExporter\n",
    "from neuralmagicML.utilsnb import (\n",
    "    KSWidgetContainer,\n",
    "    PruningEpochWidget,\n",
    "    PruningParamsWidget,\n",
    ")\n",
    "from neuralmagicML.pytorch.utils import get_named_layers_and_params_by_regex\n",
    "\n",
    "from neuralmagicML.pytorch.recal import ScheduledModifierManager, ScheduledOptimizer\n",
    "from neuralmagicML.recal import (\n",
    "    default_check_sparsities_loss,\n",
    "    default_check_sparsities_perf,\n",
    "    KSLossSensitivityAnalysis,\n",
    "    KSPerfSensitivityAnalysis,\n",
    "    KSSensitivityResult,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "check_pytorch_notebook_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'cache'\n",
    "model_name = 'bert-base-uncased'\n",
    "config = AutoConfig.from_pretrained(model_name,cache_dir=cache_dir,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir,use_fast=True,)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name,config=config,cache_dir=cache_dir,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"running approximate ks loss sensitivity analysis for model on {}\".format(device))\n",
    "\n",
    "loss_analysis = approx_ks_loss_sensitivity(model)\n",
    "\n",
    "save_path = clean_path(\n",
    "    os.path.join(\".\", notebook_name, model_name, \"ks-loss-sensitivity.json\")\n",
    ")\n",
    "loss_analysis.save_json(save_path)\n",
    "print(\"saved analysis to {}\".format(save_path))\n",
    "print(\"plotting...\")\n",
    "fig, axes = loss_analysis.plot(path=None, plot_integral=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"loss_analysis\" not in globals():\n",
    "    loss_analysis = None\n",
    "    \n",
    "param_in_scope_regex = [\"re:.*key\\.weight\", \"re:.*value\\.weight\",\"re:.*query\\.weight\",\"re:.*dense\\.weight\"]\n",
    "# match all key, value, query, and dense\n",
    "prune_layers_and_params = get_named_layers_and_params_by_regex(\n",
    "    model, param_in_scope_regex\n",
    ")\n",
    "# format to full parameter names\n",
    "param_names = [\n",
    "    \"{}.{}\".format(param.layer_name, param.param_name) for param in prune_layers_and_params\n",
    "]\n",
    "\n",
    "print(\"There are {} prunable parameters in the current selection\".format(get_n_params_by_regex(model,param_in_scope_regex)))\n",
    "widget_container = KSWidgetContainer(\n",
    "    PruningEpochWidget(start_epoch=0, end_epoch=1, total_epochs=1, max_epochs=1),\n",
    "    PruningParamsWidget(\n",
    "        param_names=param_names,\n",
    "        param_descs=[str(param.layer) for param in prune_layers_and_params],\n",
    "        param_enables=None,\n",
    "        param_sparsities=None,\n",
    "        loss_sens_analysis=loss_analysis,\n",
    "    ),\n",
    ")\n",
    "print(\"Creating ui...\")\n",
    "display(widget_container.create())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = clean_path(\"prune-config.yaml\"))\n",
    "print(\"Saving config to {}\".format(config_path)) \n",
    "widget_container.get_manager(\"pytorch\").save(config_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformernm",
   "language": "python",
   "name": "transformernm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}